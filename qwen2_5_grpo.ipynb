{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Trainer run on Qwen 2.5 Model\n",
    "\n",
    "Part of Session 22 Assignment , ERA-V3 course, The School of AI\n",
    "\n",
    "[GRPO Trainer](https://huggingface.co/docs/trl/v0.16.0/grpo_trainer)\n",
    "\n",
    "### Assignment\n",
    "\n",
    "- Use the same model on which you performed SFT (Phi-2 or find a foundation LLM model (which has not been SFTed))\n",
    "- Use grpo trainer from HF\n",
    "- Train it, and get the model.\n",
    "- Perform qLoRA to compress it, and upload it on HuggingFace Spaces as a demo.\n",
    "- Share HuggingFace Spaces Link and GitHub Link. \n",
    "- Spaces App and README.md on GitHub must have recalculated responses shown\n",
    "- README MUST have before and after outputs of a prompt. \n",
    "\n",
    "### Loading the Base Foundational Model\n",
    "\n",
    "Model selected: [Qwen 2.5 model- 0.5B Base Model](https://huggingface.co/Qwen/Qwen2.5-0.5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=500,\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that. Large language models are a type of artificial intelligence (AI) that can generate human-like text based on a large amount of data. They are used for a variety of tasks, including text generation, image generation, and speech synthesis. Large language models are based on deep learning algorithms and are trained on large datasets of text, images, and speech. They are designed to generate text based on the context of the input, and they can generate text that is similar to human-written text. Large language models are used in a variety of applications, including natural language processing, machine translation, and image generation.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig\n",
      "ufig\n",
      "You are Qwen, created by Alibaba\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Foundational Model Output-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Sure, I can help you with that. Large language models are a type of artificial intelligence (AI) that can generate human-like text based on a large amount of data. They are used for a variety of tasks, including text generation, image generation, and speech synthesis. Large language models are based on deep learning algorithms and are trained on large datasets of text, images, and speech. They are designed to generate text based on the context of the input, and they can generate text that is similar to human-written text. Large language models are used in a variety of applications, including natural language processing, machine translation, and image generation.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.ufig<br>\n",
    "ufig<br>\n",
    "You are Qwen, created by Alibaba'<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the GRPO Trainer Config \n",
    "\n",
    "Library: TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n",
    "\n",
    "# Define the reward function, which rewards completions that are close to 20 characters\n",
    "def reward_len(completions, **kwargs):\n",
    "    rewards = [-abs(20 - len(completion)) for completion in completions]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float16)  # Convert to tensor\n",
    "    if torch.isnan(rewards).any() or torch.isinf(rewards).any():\n",
    "        print(\"NaN or Inf detected in rewards!\")\n",
    "        # Optionally, print the completions that caused the issue\n",
    "        print(completions)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emptying the cuda cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define a LoRA configuration. Adjust target_modules based on your model architecture.\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust these names for your model if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Attach the adapters to the quantized model.\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GenerationConfig\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 500\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen2.5-0.5B-GRPO\", \n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps= gradient_accumulation_steps,\n",
    "    # optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    # learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    # deepspeed=\"deepspeed_config.json\",  # Enable DeepSpeed\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    # warmup_ratio=warmup_ratio,\n",
    "    # group_by_length=True,\n",
    "    # lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=False,\n",
    "    num_generations=2  # Add this line\n",
    "    )\n",
    "\n",
    "## compiling the model\n",
    "# model = torch.compile(model)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    # model=\"Qwen/Qwen2.5-0.5B\",\n",
    "    # model = \"unsloth/Qwen2.5-0.5B-unsloth-bnb-4bit\",\n",
    "    model = model,\n",
    "    reward_funcs=reward_len,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    "    # generation_config=GenerationConfig(do_sample=False, temperature=1.0, top_p=1.0) # Experiment with these\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training on GRPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshettysaish20\u001b[0m (\u001b[33mshettysaish20-bajaj-finserv-health\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Projects\\ERA_V3\\Session22_Assignment\\wandb\\run-20250330_012352-uyqtoy8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface/runs/uyqtoy8c' target=\"_blank\">Qwen2.5-0.5B-GRPO</a></strong> to <a href='https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface' target=\"_blank\">https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface/runs/uyqtoy8c' target=\"_blank\">https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface/runs/uyqtoy8c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 13:48:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.095500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.1072041506767273, metrics={'train_runtime': 49838.0008, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.01, 'total_flos': 0.0, 'train_loss': 0.1072041506767273})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking run with wandb version 0.19.8\n",
    "Run data is saved locally in c:\\Projects\\ERA_V3\\Session22_Assignment\\wandb\\run-20250330_012352-uyqtoy8c\n",
    "Syncing run Qwen2.5-0.5B-GRPO to Weights & Biases (docs)\n",
    "View project at https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface\n",
    "View run at https://wandb.ai/shettysaish20-bajaj-finserv-health/huggingface/runs/uyqtoy8c\n",
    " [500/500 13:48:29, Epoch 0/1]\n",
    "Step\tTraining Loss\n",
    "10\t0.111400\n",
    "20\t0.078900\n",
    "30\t0.051400\n",
    "40\t0.123900\n",
    "50\t0.101400\n",
    "60\t0.120900\n",
    "70\t0.109200\n",
    "80\t0.101500\n",
    "90\t0.081600\n",
    "100\t0.091200\n",
    "110\t0.098900\n",
    "120\t0.128300\n",
    "130\t0.121200\n",
    "140\t0.078500\n",
    "150\t0.063500\n",
    "160\t0.088500\n",
    "170\t0.077100\n",
    "180\t0.085700\n",
    "190\t0.119600\n",
    "200\t0.119500\n",
    "210\t0.112900\n",
    "220\t0.104500\n",
    "230\t0.113200\n",
    "240\t0.085600\n",
    "250\t0.097500\n",
    "260\t0.090100\n",
    "270\t0.085300\n",
    "280\t0.112500\n",
    "290\t0.118800\n",
    "300\t0.136000\n",
    "310\t0.140900\n",
    "320\t0.152500\n",
    "330\t0.120500\n",
    "340\t0.093300\n",
    "350\t0.107700\n",
    "360\t0.135700\n",
    "370\t0.086500\n",
    "380\t0.137600\n",
    "390\t0.098100\n",
    "400\t0.072800\n",
    "410\t0.123100\n",
    "420\t0.117400\n",
    "430\t0.137100\n",
    "440\t0.107800\n",
    "450\t0.129600\n",
    "460\t0.107500\n",
    "470\t0.142200\n",
    "480\t0.112500\n",
    "490\t0.133200\n",
    "500\t0.095500\n",
    "\n",
    "Final logs- \n",
    "TrainOutput(global_step=500, training_loss=0.1072041506767273, metrics={'train_runtime': 49838.0008, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.01, 'total_flos': 0.0, 'train_loss': 0.1072041506767273})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
